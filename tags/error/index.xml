<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>error - 标签 - 打工人日志 - jobcher</title>
        <link>https://www.jobcher.com/tags/error/</link>
        <description>error - 标签 - 打工人日志 - jobcher</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>nb@nbtyfood.com (jobcher)</managingEditor>
            <webMaster>nb@nbtyfood.com (jobcher)</webMaster><lastBuildDate>Tue, 07 Nov 2023 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://www.jobcher.com/tags/error/" rel="self" type="application/rss+xml" /><item>
    <title>linux服务器进程占用过多导致服务异常</title>
    <link>https://www.jobcher.com/system-limit/</link>
    <pubDate>Tue, 07 Nov 2023 00:00:00 &#43;0000</pubDate>
    <author>jobcher</author>
    <guid>https://www.jobcher.com/system-limit/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/images/elasticsearch.png" referrerpolicy="no-referrer">
            </div><h2 id="背景">背景</h2>
<p>我的logstash是多管道部署结果发现有大量日志丢失的情况查看logstash日志出现了以下报错</p>
<blockquote>
<p>[2023-11-07T09:43:38,492][ERROR][logstash.javapipeline    ][log] Pipeline worker error, the pipeline will be stopped {:pipeline_id=&gt;&ldquo;log&rdquo;, :error=&gt;&quot;/var/lib/logstash/queue/log/checkpoint.head.tmp (Too many open files)&quot;, :exception=&gt;Java::JavaIo::FileNotFoundException, :backtrace=&gt;[&ldquo;java.base/java.io.FileOutputStream.open0(Native Method)&rdquo;, &ldquo;java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)&rdquo;, &ldquo;java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)&rdquo;, &ldquo;java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:187)&rdquo;, &ldquo;org.logstash.ackedqueue.io.FileCheckpointIO.write(FileCheckpointIO.java:105)&rdquo;, &ldquo;org.logstash.ackedqueue.Page.headPageCheckpoint(Page.java:202)&rdquo;,</p>
</blockquote>
<p><strong>这个问题是 Logstash Pipeline 在处理数据时报错,原因是打开文件过多导致&quot;Too many open files&quot;</strong></p>
<h2 id="解决方法">解决方法</h2>
<h4 id="1-检查操作系统的文件打开数量限制使用ulimit--n查看如果太低可以提高这个限制">1. 检查操作系统的文件打开数量限制,使用<code>ulimit -n</code>查看。如果太低,可以提高这个限制</h4>
<p>打开 /etc/profile 增加ulimit 值</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">vim /etc/profile
</span></span><span class="line"><span class="cl"><span class="c1">## 增加，保存并退出</span>
</span></span><span class="line"><span class="cl"><span class="nb">ulimit</span> -n <span class="m">10240</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 重载配置</span>
</span></span><span class="line"><span class="cl"><span class="nb">source</span> /etc/profile
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="2-适当增大logstash的heap-size如-xms和-xmx设置为2g">2. 适当增大Logstash的heap size,如-Xms和-Xmx设置为2g。</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">vim /etc/logstash/jvm.option
</span></span><span class="line"><span class="cl"><span class="c1"># 修改参数</span>
</span></span><span class="line"><span class="cl">-Xms 2g
</span></span><span class="line"><span class="cl">-Xmx 2g
</span></span><span class="line"><span class="cl"><span class="c1"># 重启logstash服务</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></description>
</item>
<item>
    <title>解决Elasticsearch索引只读（read-only）</title>
    <link>https://www.jobcher.com/elasticsearch-error/</link>
    <pubDate>Wed, 20 Sep 2023 00:00:00 &#43;0000</pubDate>
    <author>jobcher</author>
    <guid>https://www.jobcher.com/elasticsearch-error/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/images/elasticsearch.png" referrerpolicy="no-referrer">
            </div><h1 id="背景">背景</h1>
<p>这两天有开发向我反馈说elasticsearch有报错，嘿，我定睛一看，这不是进入只读状态了，看来是存储达到额度，我马上加个新的数据节点，平衡一下存储压力<br>
报错信息：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">Elasticsearch Error <span class="o">{</span>type:cluster_block_exception,reason:”blocked by: <span class="o">[</span>FORBIDDEN/12/index read-only / allow delete <span class="o">(</span>api<span class="o">)]</span><span class="p">;</span><span class="o">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="新建服务器安装elasticsearch">新建服务器，安装elasticsearch</h2>
<p>为了和之前的服务器一样，我简单写一下我elasticsearch版本和服务器系统版本 </p>
<table>
<thead>
<tr>
<th style="text-align:left">软件</th>
<th style="text-align:left">版本</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">centos</td>
<td style="text-align:left">7.9</td>
</tr>
<tr>
<td style="text-align:left">elasticsearch</td>
<td style="text-align:left">6.7.2</td>
</tr>
<tr>
<td style="text-align:left">JDK</td>
<td style="text-align:left">1.8.61</td>
</tr>
<tr>
<td style="text-align:left">内存</td>
<td style="text-align:left">32G</td>
</tr>
</tbody>
</table>
<h3 id="安装和配置elasticsearch">安装和配置elasticsearch</h3>
<p>使用rpm 安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.7.2.rpm
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">rpm --install elasticsearch-6.7.2.rpm
</span></span></code></pre></td></tr></table>
</div>
</div><p>配置参数，进入<code>/etc/elasticsearch</code>目录<br>
修改配置<code>vim elasticsearch.yml</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="c"># ======================== Elasticsearch Configuration ========================= </span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">cluster.name</span><span class="p">:</span><span class="w"> </span><span class="l">cluster-prod-es</span><span class="w"> </span><span class="c"># 集群名称</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">node.name</span><span class="p">:</span><span class="w"> </span><span class="l">node-x</span><span class="w"> </span><span class="c"># 节点名称</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">path.data</span><span class="p">:</span><span class="w"> </span><span class="l">/var/lib/elasticsearch</span><span class="w"> </span><span class="c"># 数据存储</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">path.logs</span><span class="p">:</span><span class="w"> </span><span class="l">/var/log/elasticsearch </span><span class="w"> </span><span class="c"># 日志存储</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">network.host</span><span class="p">:</span><span class="w"> </span><span class="m">192.168.0.170</span><span class="w"> </span><span class="c"># 主机IP地址 </span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">http.port</span><span class="p">:</span><span class="w"> </span><span class="m">9200</span><span class="w"> </span><span class="c"># 端口号</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">discovery.zen.ping.unicast.hosts</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&#34;192.168.0.171&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;192.168.0.172&#34;</span><span class="p">,</span><span class="w"> </span><span class="s2">&#34;192.168.0.173&#34;</span><span class="p">]</span><span class="w"> </span><span class="c"># 集群节点</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">discovery.zen.minimum_master_nodes</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="c">#防止脑裂</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>修改配置<code>vim jvm.options</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl">-<span class="l">Xms16g</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>-<span class="l">Xmx16g</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>启动elasticsearch 服务</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">systemctl start elasticsearch.service
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="等待数据节点自动调节">等待数据节点自动调节</h3>
<p>这里要等待一会儿数据节点自动调节，这个调节时间取决于你数据的大小，一般来说几个小时也好了，如果数据重要性不太高的话，和领导沟通一下，就别傻坐着等他迁移完，正常下班就行了。</p>
<h3 id="关闭索引只读">关闭索引只读</h3>
<p>对了，千万不要忘记关闭只读状态，虽然你新增了节点，但是当前的只读状态并没有关闭，所以要执行一下命令关闭只读状态</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">curl -XPUT -H <span class="s2">&#34;Content-Type: application/json&#34;</span> http://192.168.0.170:9200/_all/_settings -d <span class="s1">&#39;{&#34;index.blocks.read_only_allow_delete&#34;: null}&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>或者你在head-elasticsearch上操作<br>
</p>
<h2 id="感谢你的阅读">感谢你的阅读</h2>
]]></description>
</item>
<item>
    <title>Linux 系统收包流程以及内核参数优化</title>
    <link>https://www.jobcher.com/networkinglinux/</link>
    <pubDate>Wed, 30 Aug 2023 00:00:00 &#43;0000</pubDate>
    <author>jobcher</author>
    <guid>https://www.jobcher.com/networkinglinux/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/images/networking-dc52a683.svg" referrerpolicy="no-referrer">
            </div><h2 id="简介">简介</h2>
<p>高并发的系统架构中，任何细微调整，稍有不注意便会引起连锁反应，只有系统地了解整个网络栈，在处理疑难杂症或者系统优化工作中，才能做到手中有粮心中不慌。在本节，我们概览一个 Linux 系统收包的流程，以便了解高并发系统所面临的性能瓶颈问题以及相关的优化策略。</p>
<h3 id="收包过程">收包过程</h3>
<p></p>
<ol>
<li>网卡 eth0 收到数据包。</li>
<li>网卡通过 DMA 将数据包拷贝到内存的环形缓冲区(Ring Buffer，在网卡中有 RX Ring 和 TX Ring 两种缓冲)。</li>
<li>数据从网卡拷贝到内存后, 网卡产生 IRQ（Interupt ReQuest，硬件中断）告知内核有新的数据包达到。</li>
<li>内核收到中断后, 调用相应中断处理函数，开始唤醒 ksoftirqd 内核线程处理软中断。</li>
<li>内核进行软中断处理，调用 NAPI poll 接口来获取内存环形缓冲区(ring buffer)的数据包，送至更上层处理。</li>
<li>内核中网络协议栈：L2 处理。</li>
<li>内核中网络协议栈：L3 处理。</li>
<li>内核中网络协议栈：L4 处理。</li>
<li>网络协议栈处理数据后，并将其发送到对应应用的 socket 接收缓冲区。</li>
</ol>
<h3 id="高并发瓶颈">高并发瓶颈</h3>
<ul>
<li>用户进程调用系统调用陷入内核态的开销。</li>
<li>CPU 响应包的硬中断 CPU 开销</li>
<li>ksoftirqd 内核线程的软中断上下文开销。</li>
</ul>
<h2 id="rxtx-ring-优化">RX/TX Ring 优化</h2>
<p>处理一个数据包会有各类的中断、softirq 等处理，因为分配给 Ring Buffer 的空间是有限的，当收到的数据包速率大于单个 CPU 处理速度的时，Ring Buffer 可能被占满并导致新数据包被自动丢弃。一个 CPU 去处理 Ring Buffer 数据会很低效，这个时候就产生 RSS、RPS 等多核并发机制来提升内核网络包的处理能力。</p>
<p>但是注意，<code>开启多核并发特性</code>，会挤压业务代码的执行时间，<code>如果业务属于 CPU 密集型</code>，会导致业务<code>性能下降</code>。是否开启多核处理，需要根据业务场景考虑，根据笔者的经验来看，例如此类<code>负载均衡服务器</code>、<code>网关</code>、<code>集群核心转发节点</code>等<code>网络I/O 密集型场景</code>可以尝试<code>优化 RSS、RPS 等配置</code>。</p>
<h3 id="1-判断是否需进行优化">1. 判断是否需进行优化</h3>
<p>当类似 LVS、集群核心交换节点、负载均衡服务器的场景 PPS（Packet Per Second，包每秒）指标存在一定的优化空间且这些核心节点影响了集群业务，那我们可以查看系统状态以决定是否进行内核优化。<br>
首先我们确定<code>是否存在丢包状况</code>，如果存在则进行相应的调整。<code>查询网卡收包情况</code> (RX 为收到的数据、TX 为发送的数据)。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ifconfig eth0 <span class="p">|</span> grep -E <span class="s1">&#39;RX|TX&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><code>eth0</code> 是你具体服务器的网卡地址</p>
</blockquote>
<p>出现以下结果说明RX dropped 表示数据包已经进入了 Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃，<code>RX overruns</code> 为 Ring Buffer 传输的 IO 大于 kernel 能够处理的 IO 导致，<code>overruns</code> 的错误意味着 CPU 无法即使的处理中断是造成 <code>Ring Buffer 溢出</code>。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">RX packets <span class="m">490423734</span>  bytes <span class="m">193802774970</span> <span class="o">(</span>180.4 GiB<span class="o">)</span>
</span></span><span class="line"><span class="cl">RX errors <span class="m">12732344</span>  dropped <span class="m">9008921</span>  overruns <span class="m">3723423</span>  frame <span class="m">0</span>
</span></span><span class="line"><span class="cl">TX packets <span class="m">515280693</span>  bytes <span class="m">140609362555</span> <span class="o">(</span>130.9 GiB<span class="o">)</span>
</span></span><span class="line"><span class="cl">TX errors <span class="m">0</span>  dropped <span class="m">0</span> overruns <span class="m">0</span>  carrier <span class="m">0</span>  collisions <span class="m">0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="2-rss-下的多队列调整">2. RSS 下的多队列调整</h3>
<p>RSS（receive side steering）利用网卡多队列特性，将每个核分别跟网卡的一个首发队列绑定，以达到网卡硬中断和软中断均衡的负载在各个 CPU 中，RSS 要求网卡必须要支持多队列特性。（注意：对于大部分驱动，<code>修改以下配置会使网卡先 down 再 up，因此会造成丢包</code>！）<br>
查询 RX/TX 队列配置和使用情况。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ethtool -l eth0
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">Channel parameters <span class="k">for</span> eth0:
</span></span><span class="line"><span class="cl">Pre-set maximums:
</span></span><span class="line"><span class="cl">RX:		<span class="m">0</span>
</span></span><span class="line"><span class="cl">TX:		<span class="m">0</span>
</span></span><span class="line"><span class="cl">Other:		<span class="m">0</span>
</span></span><span class="line"><span class="cl">Combined:	<span class="m">8</span>
</span></span><span class="line"><span class="cl">Current hardware settings:
</span></span><span class="line"><span class="cl">RX:		<span class="m">0</span>
</span></span><span class="line"><span class="cl">TX:		<span class="m">0</span>
</span></span><span class="line"><span class="cl">Other:		<span class="m">0</span>
</span></span><span class="line"><span class="cl">Combined:	<span class="m">4</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到硬件最多支持 6 个，当前使用了 4 个。将 RX 和 TX queue 数量都设为 8。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ethtool -L eth0 combined <span class="m">8</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="3-队列大小调整">3. 队列大小调整</h3>
<p>增大 ring buffer 可以在 PPS（packets per second）很大时<code>缓解丢包</code>问题<br>
查看队列大小。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ethtool -g eth0
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">Ring parameters <span class="k">for</span> eth0:
</span></span><span class="line"><span class="cl">Pre-set maximums:
</span></span><span class="line"><span class="cl">RX:		<span class="m">1024</span>
</span></span><span class="line"><span class="cl">RX Mini:	<span class="m">0</span>
</span></span><span class="line"><span class="cl">RX Jumbo:	<span class="m">0</span>
</span></span><span class="line"><span class="cl">TX:		<span class="m">1024</span>
</span></span><span class="line"><span class="cl">Current hardware settings:
</span></span><span class="line"><span class="cl">RX:		<span class="m">512</span>
</span></span><span class="line"><span class="cl">RX Mini:	<span class="m">0</span>
</span></span><span class="line"><span class="cl">RX Jumbo:	<span class="m">0</span>
</span></span><span class="line"><span class="cl">TX:		<span class="m">512</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>以上输出显示网卡最多支持 1024 个 RX/TX 数据包大小，但是现在只用到了 512 个。 ethtool -G 修改 queue 大小。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ethtool -G eth0 rx <span class="m">1024</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="内核协议栈优化">内核协议栈优化</h2>
<p>一个传输少量数据的 TCP 短连接的生命周期中，握手、挥手阶段将近占用了 <code>70%</code> 的资源消耗，在一个高并发的服务场景，如负载均衡、数据库等，针对性的去优化较为保守内核参数提提升服务性能的必要手段。在本文中，内核协议栈参数优化方向为 TCP 握手流程中队列大小、挥手 TIME_WAITE、keepalive 保活机制以及拥塞控制。<br>
</p>
<h3 id="tcp-握手流程参数优化">TCP 握手流程参数优化</h3>
<p>握手流程中有两个队列较为关键，当队列满时，多余的连接将会被丢弃。</p>
<ul>
<li><code>SYN Queue</code> 也被称为<code>半连接队列</code>，是内核保持的未被 ACK 的 SYN 包最大队列长度，通过内核参数 <code>net.ipv4.tcp_max_syn_backlog</code> 配置，高并发的环境下建议设置为 <code>1024 或更高</code>。</li>
<li><code>Accept Queue</code> 也被称为<code>全连接队列</code>， 是一个 socket 上等待应用程序 accept 的最大队列长度。取值为 min(backlog，net.core.somaxconn)。</li>
</ul>
<h3 id="tcp-连接保活参数优化">TCP 连接保活参数优化</h3>
<p>当 TCP 建立连接后，会有个发送一个空的 ACK 的探测行为来保持连接（keepalive）。keepalive 受以下参数影响：</p>
<ul>
<li><code>net.ipv4.tcp_keepalive_time</code> 最大闲置时间</li>
<li><code>net.ipv4.tcp_keepalive_intvl</code> 发送探测包的时间间隔</li>
<li><code>net.ipv4.tcp_keepalive_probes</code> 最大失败次数，超过此值后将通知应用层连接失效
在大规模的集群内部，如果 <code>keepalive_time</code> 设置<code>较短且发送较为频繁</code>，会产生大量的<code>空 ACK 报文</code>，存在塞满 RingBuffer 造成 TCP 丢包甚至连接断开问题。</li>
</ul>
<h3 id="tcp-连接断开参数优化">TCP 连接断开参数优化</h3>
<p>由于 <code>TCP 双全工</code>的特性，安全关闭一个连接需要四次挥手。但在一个复杂的网络环境下，会存在很多异常情况，异常断开连接会导致产生孤儿连(半连接)。 这种连接既不能发送数据，也无法接收数据，累计过多，会消耗大量系统资源。在高并发的场景下，孤儿连过多会引起资源不足，产生 <code>Address already in use: connect </code>类似的错误。<br>
<br>
挥手流程中的主要优化为 <code>TIME_WAIT</code> 的参数调整。<code>TIME_WAIT</code> 是 TCP 挥手的最后一个状态。当收到被动方发来的 FIN 报文后，主动方回复 ACK，表示确认对方的发送通道已经关闭，继而进入TIME_WAIT 状态 ，等待 2MSL 时间后关闭连接。<br>
如果发起连接一方的 <code>TIME_WAIT</code> 状态过多，会占满了所有端口资源，则会导致无法创建新连接。<code>TIME_WAIT</code> 的问题在<code>反向代理中比较明显</code>，例如 nginx 默认行为下会对于 client 传来的每一个 request 都向 upstream server 打开一个新连接，<code>高 QPS 的反向代理</code>将会快速积累 TIME_WAIT 状态的 socket，直到没有可用的本地端口，无法继续向 upstream 打开连接，此时服务将不可用。</p>
<ul>
<li><code>net.ipv4.tcp_max_tw_buckets</code>，此数值定义系统在同一时间最多能有多少 TIME_WAIT 状态，当超过这个值时，系统会<code>直接删掉这个 socket 而不会留下 TIME_WAIT 的状态</code></li>
<li><code>net.ipv4.ip_local_port_range</code>，TCP 建立连接时 client 会随机从该参数中定义的端口范围中选择一个作为源端口。可以<code>调整该参数范围增大可选择的端口范围</code>。</li>
</ul>
<h3 id="相关配置参考">相关配置参考</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">net.ipv4.tcp_tw_recycle <span class="o">=</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_tw_reuse <span class="o">=</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">net.ipv4.ip_local_port_range <span class="o">=</span> <span class="m">1024</span> <span class="m">65535</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_rmem <span class="o">=</span> <span class="m">16384</span> <span class="m">262144</span> <span class="m">8388608</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_wmem <span class="o">=</span> <span class="m">32768</span> <span class="m">524288</span> <span class="m">16777216</span>
</span></span><span class="line"><span class="cl">net.core.somaxconn <span class="o">=</span> <span class="m">8192</span>
</span></span><span class="line"><span class="cl">net.core.rmem_max <span class="o">=</span> <span class="m">16777216</span>
</span></span><span class="line"><span class="cl">net.core.wmem_max <span class="o">=</span> <span class="m">16777216</span>
</span></span><span class="line"><span class="cl">net.core.wmem_default <span class="o">=</span> <span class="m">2097152</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_max_tw_buckets <span class="o">=</span> <span class="m">5000</span>
</span></span><span class="line"><span class="cl">net.ipv4.tcp_max_syn_backlog <span class="o">=</span> <span class="m">10240</span>
</span></span><span class="line"><span class="cl">net.core.netdev_max_backlog <span class="o">=</span> <span class="m">10240</span>
</span></span><span class="line"><span class="cl">net.netfilter.nf_conntrack_max <span class="o">=</span> <span class="m">1000000</span>
</span></span><span class="line"><span class="cl">net.ipv4.netfilter.ip_conntrack_tcp_timeout_established <span class="o">=</span> <span class="m">7200</span>
</span></span><span class="line"><span class="cl">net.core.default_qdisc <span class="o">=</span> fq_codel
</span></span><span class="line"><span class="cl">net.ipv4.tcp_congestion_control <span class="o">=</span> bbr
</span></span><span class="line"><span class="cl">net.ipv4.tcp_slow_start_after_idle <span class="o">=</span> <span class="m">0</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></description>
</item>
<item>
    <title>Vue3 &#43; vite &#43; nginx项目部署后404问题</title>
    <link>https://www.jobcher.com/nginx-error/</link>
    <pubDate>Thu, 10 Aug 2023 00:00:00 &#43;0000</pubDate>
    <author>jobcher</author>
    <guid>https://www.jobcher.com/nginx-error/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/images/nginx.png" referrerpolicy="no-referrer">
            </div><h2 id="vue3--vite--nginx项目部署后404问题">Vue3 + vite + nginx项目部署后404问题</h2>
<p><code>vue3 + vite + nginx</code><br>
在服务器上部署后打开首页都没问题，打开其他路径全部 404。<br>
<code>nginx</code> 报错日志：<code>No such file or directory</code></p>
<blockquote>
<p>其实查看 <code>build</code> 后的<code>dist文件夹</code>可以发现，只有一个<code>index.html</code>，当你访问别的路径时<code>nignx</code>查找不到所以就报错了</p>
</blockquote>
<h2 id="解决方案">解决方案</h2>
<p>在 nginx.conf 中添加: <code>try_files $uri $uri/ /index.html;</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">server {
</span></span><span class="line"><span class="cl">          listen       80;
</span></span><span class="line"><span class="cl">          server_name  localhost;
</span></span><span class="line"><span class="cl">          location / {
</span></span><span class="line"><span class="cl">          root   /dist;
</span></span><span class="line"><span class="cl">          index  index.html index.htm;
</span></span><span class="line"><span class="cl">          # 在配置文件的此处加上这句话
</span></span><span class="line"><span class="cl">          try_files $uri $uri/ /index.html;
</span></span><span class="line"><span class="cl">        }
</span></span><span class="line"><span class="cl">    }
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="总结">总结</h2>
<p>其实上述改动就是告诉 nignx 找不到文件的时候就访问 <code>index.html</code> 就可以了。</p>
<p>究其原因其实就是是 vue3 的 router 使用了<code>history</code>模式，该模式与之前<code>hash</code>模式的具体区别可以自行百度一下，不在此赘述。</p>
]]></description>
</item>
<item>
    <title>githubAction set-output弃用错误</title>
    <link>https://www.jobcher.com/github-error/</link>
    <pubDate>Fri, 21 Oct 2022 00:00:00 &#43;0000</pubDate>
    <author>jobcher</author>
    <guid>https://www.jobcher.com/github-error/</guid>
    <description><![CDATA[<h1 id="githubaction-set-output-弃用错误">githubAction set-output 弃用错误</h1>
<p>The <code>set-output</code> command is deprecated and will be disabled soon. Please upgrade to using Environment Files. For more information see: <a href="https://github.blog/changelog/2022-10-11-github-actions-deprecating-save-state-and-set-output-commands/" target="_blank" rel="noopener noreffer ">https://github.blog/changelog/2022-10-11-github-actions-deprecating-save-state-and-set-output-commands/</a></p>
<h2 id="原因">原因</h2>
<p>如果您有一个使用 设置输出的<code>GitHub Actionsecho ::set-output key=value</code>工作流程，您已经开始看到无用的弃用警告。这是修复它的方法。查看官方链接基本上得不到什么帮助！</p>
<h2 id="修复方法">修复方法</h2>
<ol>
<li>更新其它人的 action 方法</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">将 @actions/core 提升到 1.10.0
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>修改自己的 aciton 方法</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">run: <span class="nb">echo</span> <span class="s2">&#34;::set-output name=KEY::VALUE&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1">## 改为</span>
</span></span><span class="line"><span class="cl">run: <span class="nb">echo</span> <span class="s2">&#34;KEY=VALUE&#34;</span> &gt;&gt;<span class="nv">$GITHUB_OUTPUT</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>建议：使用自己的方法</p>
</blockquote>
<h2 id="总结">总结</h2>
<p>平台经营者非常肆意妄为的修改自己的代码内容弃用功能，无限的权力滋生傲慢……我相信大部分开发这并没有注意到这个告警，知道流水线服务报错之后才会注意到，希望微软可以对能更加包容不同的开发者，尊重开发者社区。</p>
]]></description>
</item>
<item>
    <title>k8s CNI 问题 连接认证失效</title>
    <link>https://www.jobcher.com/k8s-error2/</link>
    <pubDate>Fri, 23 Sep 2022 00:00:00 &#43;0000</pubDate>
    <author>jobcher</author>
    <guid>https://www.jobcher.com/k8s-error2/</guid>
    <description><![CDATA[<h1 id="k8s-cni-问题-连接认证失效">k8s CNI 问题 连接认证失效</h1>
<p>删除 <code>calico</code> 换成 <code>flannel</code> 后，容器没有正常启动<br>
network: error getting ClusterInformation: connection is unauthorized: Unauthorized]</p>
<h2 id="解决问题">解决问题</h2>
<p>删除掉 <code>/etc/cni/net.d/</code> 目录下的 calico 配置文件即可。<br>
要删除<code>所有节点</code>的配置文件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">sudo rm -rf /etc/cni/net.d/*calico*
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="不要重复网络插件">不要重复网络插件</h2>
<p></p>
]]></description>
</item>
<item>
    <title>k8s.gcr.io国内无法连接解决方法</title>
    <link>https://www.jobcher.com/k8s-error3/</link>
    <pubDate>Thu, 22 Sep 2022 00:00:00 &#43;0000</pubDate>
    <author>jobcher</author>
    <guid>https://www.jobcher.com/k8s-error3/</guid>
    <description><![CDATA[<h1 id="k8sgcrio-国内无法连接解决方法">k8s.gcr.io 国内无法连接解决方法</h1>
<p>Get <a href="https://k8s.gcr.io/v2/" target="_blank" rel="noopener noreffer ">https://k8s.gcr.io/v2/</a>: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)<br>
这个一看知道什么原因了，应该 GFW！那好吧，只能给 docker 加个代理了。</p>
<h2 id="解决问题">解决问题</h2>
<p>添加 mirror 站点</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">registry.cn-hangzhou.aliyuncs.com/google_containers
</span></span></code></pre></td></tr></table>
</div>
</div>]]></description>
</item>
<item>
    <title>K8S 问题排查：cgroup 内存泄露问题</title>
    <link>https://www.jobcher.com/k8s-error/</link>
    <pubDate>Tue, 30 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>jobcher</author>
    <guid>https://www.jobcher.com/k8s-error/</guid>
    <description><![CDATA[<h1 id="k8s-问题排查cgroup-内存泄露问题">K8S 问题排查：cgroup 内存泄露问题</h1>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">unable to ensure pod container exists: failed to create container for [kubepods besteffort pod5f26dae8-0421-4eab-a3f7-aa51c6848e2b] : mkdir /sys/fs/cgroup/memory/kubepods/besteffort/pod5f26dae8-0421-4eab-a3f7-aa51c6848e2b: cannot allocate memory
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="查看-linux-内核">查看 linux 内核</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">cat /proc/version
</span></span><span class="line"><span class="cl">uname -a
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以发现 linux 版本是 3.0 版本</p>
<h2 id="原因">原因</h2>
<blockquote>
<p><code>cgroup</code> 的 <code>kmem account</code> 特性在 <code>Linux 3.x</code> 内核上有内存泄露问题，然后<code>k8s</code>用了这个特性，导致后面创建不出新的<code>pod</code>来了</p>
</blockquote>
<h2 id="解决方法">解决方法</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># 修改/etc/default/grub 为</span>
</span></span><span class="line"><span class="cl"><span class="nv">GRUB_CMDLINE_LINUX</span><span class="o">=</span><span class="s2">&#34;crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet cgroup.memory=nokmem&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1">#加上了 cgroup.memory=nokmem</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 生成配置</span>
</span></span><span class="line"><span class="cl">/usr/sbin/grub2-mkconfig -o /boot/grub2/grub.cfg
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 重启机器</span>
</span></span><span class="line"><span class="cl">reboot
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="验证">验证</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">cat /sys/fs/cgroup/memory/kubepods/burstable/pod*/*/memory.kmem.slabinfo
</span></span></code></pre></td></tr></table>
</div>
</div><p>输出信息</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">cat: /sys/fs/cgroup/memory/kubepods/burstable/pod0fe273ca-42e0-4223-9fe8-16d8dd1774e9/0fdd5d9c16929fd600dbdf313b5c3ebabad912dc0cb076ed6e7799e028b31481/memory.kmem.slabinfo: 输入/输出错误
</span></span><span class="line"><span class="cl">cat: /sys/fs/cgroup/memory/kubepods/burstable/pod0fe273ca-42e0-4223-9fe8-16d8dd1774e9/aa30198d0c5413b70bf488c9daa350a85c7fc6b677235c5adaf2dde6caf95ec4/memory.kmem.slabinfo: 输入/输出错误
</span></span><span class="line"><span class="cl">cat: /sys/fs/cgroup/memory/kubepods/burstable/pod5be86c5d-d012-4cc2-b693-4882a15eda90/059b26b00f4f286b0f52e759b83dad79c7676e1705ee0f3f175a277fd1e5ea5a/memory.kmem.slabinfo: 输入/输出错误
</span></span><span class="line"><span class="cl">cat: /sys/fs/cgroup/memory/kubepods/burstable/pod5be86c5d-d012-4cc2-b693-4882a15eda90/bfa9db0c23fd056a0c05ee5b2b377dd551451cc0f18ddd5db82f9693674a4677/memory.kmem.slabinfo: 输入/输出错误
</span></span><span class="line"><span class="cl">cat: /sys/fs/cgroup/memory/kubepods/burstable/pod8f588e3d-fe89-4716-ab36-3ef606c70367/6fab9f4f7a83bf4c79a68277b214807bd566a8f13212a0fdb5742e4eee4d75d5/memory.kmem.slabinfo: 输入/输出错误
</span></span><span class="line"><span class="cl">cat: /sys/fs/cgroup/memory/kubepods/burstable/pod8f588e3d-fe89-4716-ab36-3ef606c70367/b04594732f7e38a47500ffe1150705110cfa683b585aa7eaf0965cc48ba2a46d/memory.kmem.slabinfo: 输入/输出错误
</span></span><span class="line"><span class="cl">cat: /sys/fs/cgroup/memory/kubepods/burstable/pod9c449bdf-492b-4adc-a623-4ced323ac6d4/c9e73e56ddae0c6c301e43852a51165419eb293b05fa65407f8cb3fe449daf5d/memory.kmem.slabinfo: 输入/输出错误
</span></span><span class="line"><span class="cl">cat: /sys/fs/cgroup/memory/kubepods/burstable/pod9c449bdf-492b-4adc-a623-4ced323ac6d4/d7392b1e4be1728fd739dc7117e6efc723a4727f143b91a1b386ad35dc1d3a2e/memory.kmem.slabinfo: 输入/输出错误
</span></span><span class="line"><span class="cl">cat: /sys/fs/cgroup/memory/kubepods/burstable/podd316acd7-69fe-4ad5-963a-6e19174b7cb0/0ec18ac0509e6ab454ebe637bde002330afc9eb70eff6f23fe8caa12880e82f6/memory.kmem.slabinfo: 输入/输出错误
</span></span><span class="line"><span class="cl">cat: /sys/fs/cgroup/memory/kubepods/burstable/podd316acd7-69fe-4ad5-963a-6e19174b7cb0/dc5be82c01802c349c9505375c37dc054898b9b84e57cb4e671044e8a6459aac/memory.kmem.slabinfo: 输入/输出错误
</span></span></code></pre></td></tr></table>
</div>
</div>]]></description>
</item>
<item>
    <title>docker 问题处理</title>
    <link>https://www.jobcher.com/docker-error/</link>
    <pubDate>Thu, 04 Aug 2022 00:00:00 &#43;0000</pubDate>
    <author>jobcher</author>
    <guid>https://www.jobcher.com/docker-error/</guid>
    <description><![CDATA[<h3 id="docker-无法启动">docker 无法启动</h3>
<p>打开服务器输入<code>docker ps</code>,输出错误<br>
Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?</p>
<p>怀疑是不是<code>docker.services</code> 部署没成功，<code>systemctl start docker</code> 启动 docker，结果服务器还是报错</p>
<p>Job for docker.service failed because the control process exited with error code.<br>
See &ldquo;systemctl status docker.service&rdquo; and &ldquo;journalctl -xe&rdquo; for details.</p>
<p><code>systemctl status docker.service</code> 输出日志：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">● docker.service - Docker Application Container Engine
</span></span><span class="line"><span class="cl">     Loaded: loaded <span class="o">(</span>/lib/systemd/system/docker.service<span class="p">;</span> enabled<span class="p">;</span> vendor preset: enabled<span class="o">)</span>
</span></span><span class="line"><span class="cl">     Active: failed <span class="o">(</span>Result: exit-code<span class="o">)</span> since Thu 2022-08-04 11:43:05 CST<span class="p">;</span> 2min 57s ago
</span></span><span class="line"><span class="cl">TriggeredBy: ● docker.socket
</span></span><span class="line"><span class="cl">       Docs: https://docs.docker.com
</span></span><span class="line"><span class="cl">    Process: <span class="m">30432</span> <span class="nv">ExecStart</span><span class="o">=</span>/usr/bin/dockerd -H fd:// --containerd<span class="o">=</span>/run/containerd/containerd.sock <span class="o">(</span><span class="nv">code</span><span class="o">=</span>exited, <span class="nv">status</span><span class="o">=</span>1/FAILURE<span class="o">)</span>
</span></span><span class="line"><span class="cl">   Main PID: <span class="m">30432</span> <span class="o">(</span><span class="nv">code</span><span class="o">=</span>exited, <span class="nv">status</span><span class="o">=</span>1/FAILURE<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Aug <span class="m">04</span> 11:43:05 master01 systemd<span class="o">[</span>1<span class="o">]</span>: docker.service: Scheduled restart job, restart counter is at 3.
</span></span><span class="line"><span class="cl">Aug <span class="m">04</span> 11:43:05 master01 systemd<span class="o">[</span>1<span class="o">]</span>: Stopped Docker Application Container Engine.
</span></span><span class="line"><span class="cl">Aug <span class="m">04</span> 11:43:05 master01 systemd<span class="o">[</span>1<span class="o">]</span>: docker.service: Start request repeated too quickly.
</span></span><span class="line"><span class="cl">Aug <span class="m">04</span> 11:43:05 master01 systemd<span class="o">[</span>1<span class="o">]</span>: docker.service: Failed with result <span class="s1">&#39;exit-code&#39;</span>.
</span></span><span class="line"><span class="cl">Aug <span class="m">04</span> 11:43:05 master01 systemd<span class="o">[</span>1<span class="o">]</span>: Failed to start Docker Application Container Engine.
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>journalctl -xe</code> 输出日志：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">Aug <span class="m">04</span> 11:46:49 master01 systemd<span class="o">[</span>1<span class="o">]</span>: Starting Docker Socket <span class="k">for</span> the API.
</span></span><span class="line"><span class="cl">-- Subject: A start job <span class="k">for</span> unit docker.socket has begun execution
</span></span><span class="line"><span class="cl">-- Defined-By: systemd
</span></span><span class="line"><span class="cl">-- Support: http://www.ubuntu.com/support
</span></span><span class="line"><span class="cl">--
</span></span><span class="line"><span class="cl">-- A start job <span class="k">for</span> unit docker.socket has begun execution.
</span></span><span class="line"><span class="cl">--
</span></span><span class="line"><span class="cl">-- The job identifier is 58900.
</span></span><span class="line"><span class="cl">Aug <span class="m">04</span> 11:46:49 master01 systemd<span class="o">[</span>1<span class="o">]</span>: Listening on Docker Socket <span class="k">for</span> the API.
</span></span><span class="line"><span class="cl">-- Subject: A start job <span class="k">for</span> unit docker.socket has finished successfully
</span></span><span class="line"><span class="cl">-- Defined-By: systemd
</span></span><span class="line"><span class="cl">-- Support: http://www.ubuntu.com/support
</span></span><span class="line"><span class="cl">--
</span></span><span class="line"><span class="cl">-- A start job <span class="k">for</span> unit docker.socket has finished successfully.
</span></span><span class="line"><span class="cl">--
</span></span><span class="line"><span class="cl">-- The job identifier is 58900.
</span></span><span class="line"><span class="cl">Aug <span class="m">04</span> 11:46:49 master01 systemd<span class="o">[</span>1<span class="o">]</span>: Starting Docker Application Container Engine...
</span></span><span class="line"><span class="cl">-- Subject: A start job <span class="k">for</span> unit docker.service has begun execution
</span></span><span class="line"><span class="cl">-- Defined-By: systemd
</span></span><span class="line"><span class="cl">-- Support: http://www.ubuntu.com/support
</span></span><span class="line"><span class="cl">--
</span></span><span class="line"><span class="cl">-- A start job <span class="k">for</span> unit docker.service has begun execution.
</span></span><span class="line"><span class="cl">--
</span></span><span class="line"><span class="cl">-- The job identifier is 58830.
</span></span><span class="line"><span class="cl">Aug <span class="m">04</span> 11:46:49 master01 dockerd<span class="o">[</span>30544<span class="o">]</span>: unable to configure the Docker daemon with file /etc/docker/daemon.json: EOF
</span></span><span class="line"><span class="cl">Aug <span class="m">04</span> 11:46:49 master01 systemd<span class="o">[</span>1<span class="o">]</span>: docker.service: Main process exited, <span class="nv">code</span><span class="o">=</span>exited, <span class="nv">status</span><span class="o">=</span>1/FAILURE
</span></span><span class="line"><span class="cl">-- Subject: Unit process exited
</span></span><span class="line"><span class="cl">-- Defined-By: systemd
</span></span><span class="line"><span class="cl">-- Support: http://www.ubuntu.com/support
</span></span><span class="line"><span class="cl">--
</span></span><span class="line"><span class="cl">-- An <span class="nv">ExecStart</span><span class="o">=</span> process belonging to unit docker.service has exited.
</span></span><span class="line"><span class="cl">--
</span></span><span class="line"><span class="cl">-- The process<span class="s1">&#39; exit code is &#39;</span>exited<span class="s1">&#39; and its exit status is 1.
</span></span></span><span class="line"><span class="cl"><span class="s1">Aug 04 11:46:49 master01 systemd[1]: docker.service: Failed with result &#39;</span>exit-code<span class="s1">&#39;.
</span></span></span><span class="line"><span class="cl"><span class="s1">-- Subject: Unit failed
</span></span></span><span class="line"><span class="cl"><span class="s1">-- Defined-By: systemd
</span></span></span><span class="line"><span class="cl"><span class="s1">-- Support: http://www.ubuntu.com/support
</span></span></span><span class="line"><span class="cl"><span class="s1">--
</span></span></span><span class="line"><span class="cl"><span class="s1">-- The unit docker.service has entered the &#39;</span>failed<span class="s1">&#39; state with result &#39;</span>exit-code<span class="err">&#39;</span>.
</span></span><span class="line"><span class="cl">Aug <span class="m">04</span> 11:46:49 master01 systemd<span class="o">[</span>1<span class="o">]</span>: Failed to start Docker Application Container Engine.
</span></span><span class="line"><span class="cl">-- Subject: A start job <span class="k">for</span> unit docker.service has failed
</span></span><span class="line"><span class="cl">-- Defined-By: systemd
</span></span><span class="line"><span class="cl">-- Support: http://www.ubuntu.com/support
</span></span><span class="line"><span class="cl">--
</span></span><span class="line"><span class="cl">-- A start job <span class="k">for</span> unit docker.service has finished with a failure.
</span></span><span class="line"><span class="cl">--
</span></span><span class="line"><span class="cl">-- The job identifier is <span class="m">58830</span> and the job result is failed.
</span></span><span class="line"><span class="cl">Aug <span class="m">04</span> 11:46:49 master01 sudo<span class="o">[</span>30535<span class="o">]</span>: pam_unix<span class="o">(</span>sudo:session<span class="o">)</span>: session closed <span class="k">for</span> user root
</span></span></code></pre></td></tr></table>
</div>
</div><p>我运行了:</p>
<blockquote>
<p>sudo dockerd &ndash;debug</p>
</blockquote>
<p>输出日志：<br>
unable to configure the Docker daemon with file /etc/docker/daemon.json: EOF</p>
<ul>
<li>解决方法：</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1">#如果 /etc/docker/daemon.json 为空</span>
</span></span><span class="line"><span class="cl">vim /etc/docker/daemon.json
</span></span><span class="line"><span class="cl"><span class="c1"># 添加</span>
</span></span><span class="line"><span class="cl"><span class="o">{</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">#保存退出</span>
</span></span><span class="line"><span class="cl">:wq
</span></span></code></pre></td></tr></table>
</div>
</div><p>重启 docker</p>
<blockquote>
<p>systemctl restart docker</p>
</blockquote>
]]></description>
</item>
<item>
    <title>ProXmoX VE升级 apt-get update 报错</title>
    <link>https://www.jobcher.com/pveupdate/</link>
    <pubDate>Sun, 30 Jan 2022 00:00:00 &#43;0000</pubDate>
    <author>jobcher</author>
    <guid>https://www.jobcher.com/pveupdate/</guid>
    <description><![CDATA[<h1 id="proxmox-ve-升级-apt-get-update-报错">ProXmoX VE 升级 apt-get update 报错</h1>
<h2 id="解决方法">解决方法</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">vim /etc/apt/sources.list.d/pve-enterprise.list
</span></span><span class="line"><span class="cl"><span class="c1">#注释掉</span>
</span></span><span class="line"><span class="cl"><span class="c1">#deb https://enterprise.proxmox.com/debian/pve stretch pve-enterprise</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="添加内容">添加内容</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;deb http://download.proxmox.com/debian/pve stretch pve-no-subscription&#34;</span> &gt; /etc/apt/sources.list.d/pve-install-repo.list
</span></span><span class="line"><span class="cl">wget http://download.proxmox.com/debian/proxmox-ve-release-5.x.gpg -O /etc/apt/trusted.gpg.d/proxmox-ve-release-5.x.gpg
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="更新系统">更新系统</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">apt update <span class="o">&amp;&amp;</span> apt dist-upgrade
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="结尾">结尾</h2>
<p>升级完成后，可以执行<code>pveversion -v</code>查看下最新的软件版本。然后执行<code>reboot</code>重启物理服务器</p>
]]></description>
</item>
</channel>
</rss>
